{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbca3df",
   "metadata": {},
   "source": [
    "# AWS Credentials Required\n",
    "\n",
    "> To run this notebook, you must set AWS credentials in your environment before executing any cells.\n",
    ">\n",
    "- `AWS_ACCESS_KEY_ID`\n",
    "- `AWS_SECRET_ACCESS_KEY`\n",
    "- `AWS_SESSION_TOKEN` (only if using temporary credentials)\n",
    "- `AWS_REGION` (optional; defaults to `us-east-1`)\n",
    "\n",
    "> Tip: You can set these in your terminal session or in a `.env` file loaded by the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c6328",
   "metadata": {},
   "source": [
    "# Backblaze Data Pipeline - S3 Setup and Data Loading\n",
    "\n",
    "This notebook sets up an S3 bucket, downloads Backblaze hard drive data, and converts it to partitioned Parquet format for efficient querying.\n",
    "\n",
    "## 1. Initialize S3 Bucket\n",
    "\n",
    "Creates or retrieves an S3 bucket for storing raw and curated data. The bucket name is persisted in a `.env` file for reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43983a8e-a5ca-49f5-9ee6-57d4dfc8e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihamdan/Library/Python/3.9/lib/python/site-packages/boto3/compat.py:89: PythonDeprecationWarning: Boto3 will no longer support Python 3.9 starting April 29, 2026. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.10 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new bucket: mlops-backblaze-83d6dd0d-us-east-1\n",
      "Error: AWS credentials not found.\n",
      "Running in local-only mode. S3 steps will be skipped.\n",
      "Local mode: True\n",
      "Bucket not created due to missing credentials.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import boto3\n",
    "from dotenv import load_dotenv, set_key\n",
    "\n",
    "ENV_PATH = \".env\"\n",
    "\n",
    "# Load .env if it exists\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\", \"/tmp/aai540\")\n",
    "LOCAL_ZIP_DIR = os.path.join(DATA_DIR, \"backblaze_zips\")\n",
    "LOCAL_PARQUET_DIR = os.path.join(DATA_DIR, \"backblaze_parquet\")\n",
    "os.makedirs(LOCAL_ZIP_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "# If BUCKET_NAME exists, use it\n",
    "if bucket:\n",
    "    print(\"Using existing bucket from .env:\", bucket)\n",
    "\n",
    "else:\n",
    "    # Create new bucket name\n",
    "    bucket = f\"mlops-backblaze-{uuid.uuid4().hex[:8]}-{region}\"\n",
    "    print(\"Creating new bucket:\", bucket)\n",
    "\n",
    "    try:\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket,\n",
    "                CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "            )\n",
    "    except Exception as e:\n",
    "        if \"NoCredentialsError\" in str(type(e)):\n",
    "            print(\"Error: AWS credentials not found.\")\n",
    "            print(\"Running in local-only mode. S3 steps will be skipped.\")\n",
    "            bucket = None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    if bucket is not None:\n",
    "        # Persist to .env\n",
    "        if not os.path.exists(ENV_PATH):\n",
    "            open(ENV_PATH, \"w\").close()\n",
    "\n",
    "        set_key(ENV_PATH, \"BUCKET_NAME\", bucket)\n",
    "\n",
    "local_mode = bucket is None\n",
    "print(\"Local mode:\", local_mode)\n",
    "if bucket:\n",
    "    print(\"Bucket ready:\", bucket)\n",
    "else:\n",
    "    print(\"Bucket not created due to missing credentials.\")\n",
    "reload_s3 = False if local_mode else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ee59",
   "metadata": {},
   "source": [
    "## 2. Create Folder Structure\n",
    "\n",
    "Creates a standard MLOps folder structure in S3 to organize:\n",
    "- Raw data (Backblaze hard drive stats, reviews)\n",
    "- Curated/processed data\n",
    "- Feature stores\n",
    "- Model artifacts\n",
    "- Evaluation results\n",
    "- Monitoring data\n",
    "- Batch inference outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d4fa7e-bd95-46ee-bcaa-a3344a97f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: using /tmp/aai540\n"
     ]
    }
   ],
   "source": [
    "if not local_mode:\n",
    "    prefixes = [\n",
    "        \"raw/backblaze/\",\n",
    "        \"raw/reviews/\",\n",
    "        \"curated/\",\n",
    "        \"features/\",\n",
    "        \"artifacts/models/\",\n",
    "        \"artifacts/eval/\",\n",
    "        \"artifacts/monitoring/\",\n",
    "        \"inference/batch/\"\n",
    "    ]\n",
    "\n",
    "    for p in prefixes:\n",
    "        s3.put_object(Bucket=bucket, Key=p, Body=b'')\n",
    "\n",
    "    print(\"Folder layout created:\")\n",
    "    for p in prefixes:\n",
    "        print(\" -\", p)\n",
    "else:\n",
    "    print(\"Local mode: using\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21545a3a",
   "metadata": {},
   "source": [
    "## 3. Verify Folder Structure\n",
    "\n",
    "Lists all objects in the S3 bucket to confirm the folder structure was created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5e3eae-8202-4219-a984-5101e9bf9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: listing local directories\n",
      "ZIP dir: /tmp/aai540/backblaze_zips\n",
      "Parquet dir: /tmp/aai540/backblaze_parquet\n"
     ]
    }
   ],
   "source": [
    "if not local_mode:\n",
    "    resp = s3.list_objects_v2(Bucket=bucket)\n",
    "    for obj in resp.get(\"Contents\", []):\n",
    "        print(obj[\"Key\"])\n",
    "else:\n",
    "    print(\"Local mode: listing local directories\")\n",
    "    print(\"ZIP dir:\", LOCAL_ZIP_DIR)\n",
    "    print(\"Parquet dir:\", LOCAL_PARQUET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9f2c7",
   "metadata": {},
   "source": [
    "## 4. Download Backblaze Data to S3\n",
    "\n",
    "Scrapes the Backblaze website to find all quarterly data files from 2024 onwards, then:\n",
    "- Downloads each ZIP file directly from Backblaze\n",
    "- Streams the data to S3 without storing locally (efficient for large files)\n",
    "- Skips files that already exist in S3 to avoid redundant downloads\n",
    "- Stores ZIP files in `raw/backblaze/zips/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a20ee4-0351-4641-95f6-ef4301917a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URLs: 7\n",
      "First 5: ['https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2024.zip', 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2025.zip', 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2024.zip', 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2025.zip', 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2024.zip']\n",
      "Downloading: https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2024.zip\n",
      "Saved sample parquet: /tmp/aai540/engineered_data_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape Backblaze URLs with user-agent to avoid 403 Forbidden\n",
    "req = Request(\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data\", headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})\n",
    "html = urlopen(req).read().decode(\"utf-8\")\n",
    "urls = sorted(set(re.findall(r\"https://f001\\.backblazeb2\\.com/file/Backblaze-Hard-Drive-Data/data_Q[1-4]_[0-9]{4}\\.zip\", html)))\n",
    "urls = [u for u in urls if re.search(r\"_202[4-9]\\.zip\", u)]\n",
    "\n",
    "print(\"Found URLs:\", len(urls))\n",
    "print(\"First 5:\", urls[:5])\n",
    "\n",
    "if not urls:\n",
    "    raise ValueError(\"No Backblaze URLs found.\")\n",
    "\n",
    "# Download first ZIP locally for a sample parquet\n",
    "sample_url = urls[0]\n",
    "fname = os.path.basename(sample_url)\n",
    "local_zip = os.path.join(LOCAL_ZIP_DIR, fname)\n",
    "if not os.path.exists(local_zip):\n",
    "    print(\"Downloading:\", sample_url)\n",
    "    data = urlopen(sample_url).read()\n",
    "    with open(local_zip, \"wb\") as f:\n",
    "        f.write(data)\n",
    "else:\n",
    "    print(\"Using cached ZIP:\", local_zip)\n",
    "\n",
    "# Optionally upload to S3 if available\n",
    "if not local_mode:\n",
    "    try:\n",
    "        s3.upload_file(local_zip, bucket, f\"raw/backblaze/zips/{fname}\")\n",
    "        print(\"Uploaded to S3:\", f\"s3://{bucket}/raw/backblaze/zips/{fname}\")\n",
    "    except Exception as e:\n",
    "        print(\"S3 upload skipped:\", e)\n",
    "\n",
    "# Create a local sample parquet for downstream notebooks\n",
    "with zipfile.ZipFile(local_zip, \"r\") as z:\n",
    "    csv_files = [n for n in z.namelist() if n.endswith(\".csv\") and not os.path.basename(n).startswith(\"._\")]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files found in ZIP.\")\n",
    "    sample_csv = csv_files[0]\n",
    "    with z.open(sample_csv) as f:\n",
    "        df_sample = pd.read_csv(f, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "sample_parquet = os.path.join(DATA_DIR, \"engineered_data_sample.parquet\")\n",
    "df_sample.to_parquet(sample_parquet, index=False)\n",
    "print(\"Saved sample parquet:\", sample_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f88a5",
   "metadata": {},
   "source": [
    "## 5. List Downloaded ZIP Files\n",
    "\n",
    "Verifies that all ZIP files were successfully uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246621ee-6c6b-449a-a3c2-41800c04fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 13:42:10 1020483699 data_Q1_2025.zip\n",
      "2026-01-25 13:43:05 1067562257 data_Q2_2025.zip\n",
      "2026-01-25 13:44:03 1111587745 data_Q3_2025.zip\n"
     ]
    }
   ],
   "source": [
    "if not local_mode:\n",
    "    import subprocess\n",
    "    result = subprocess.run([\"aws\", \"s3\", \"ls\", f\"s3://{bucket}/raw/backblaze/zips/\"], capture_output=True, text=True)\n",
    "    print(result.stdout or result.stderr)\n",
    "else:\n",
    "    print(\"Local ZIP files:\")\n",
    "    for name in sorted(os.listdir(LOCAL_ZIP_DIR))[:10]:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b966074",
   "metadata": {},
   "source": [
    "## 6. List and Prepare ZIP Files for Processing\n",
    "\n",
    "Imports libraries for data processing and lists all ZIP files from S3 that need to be converted to Parquet format. Uses pagination to handle large numbers of files efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffcf69-ed43-439c-bae0-e1ad343374ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIPs found: 3\n",
      "['raw/backblaze/zips/data_Q1_2025.zip', 'raw/backblaze/zips/data_Q2_2025.zip', 'raw/backblaze/zips/data_Q3_2025.zip']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_s3_prefix = \"raw/backblaze/zips/\"\n",
    "out_prefix = \"curated/backblaze_parquet/\"\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    keys = []\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            if obj[\"Key\"].endswith(\".zip\"):\n",
    "                keys.append(obj[\"Key\"])\n",
    "    return sorted(keys)\n",
    "\n",
    "if local_mode:\n",
    "    zip_keys = [os.path.join(LOCAL_ZIP_DIR, f) for f in os.listdir(LOCAL_ZIP_DIR) if f.endswith(\".zip\")]\n",
    "else:\n",
    "    zip_keys = list_s3_keys(zip_s3_prefix)\n",
    "\n",
    "print(\"ZIPs found:\", len(zip_keys))\n",
    "print(zip_keys[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fbf59",
   "metadata": {},
   "source": [
    "## 7. Convert CSV to Partitioned Parquet\n",
    "\n",
    "Processes each ZIP file by:\n",
    "1. Downloading the ZIP from S3 into memory\n",
    "2. Extracting CSV files from the ZIP\n",
    "3. Parsing date information from CSV filenames (YYYY-MM-DD)\n",
    "4. Converting CSV to Parquet format with Snappy compression\n",
    "5. Uploading to S3 with Hive-style partitioning: `year=YYYY/month=MM/day=DD/`\n",
    "6. Skipping files that already exist to enable resumable processing\n",
    "\n",
    "This partitioned structure enables efficient querying by date ranges in tools like Athena or Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "289feb89-baac-45f0-be3c-4a88961a24d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: sample parquet already saved to /tmp/aai540/engineered_data_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "if local_mode:\n",
    "    print(\"Local mode: sample parquet already saved to\", os.path.join(DATA_DIR, \"engineered_data_sample.parquet\"))\n",
    "else:\n",
    "    def upload_parquet_to_s3(df, s3_key):\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        buf = BytesIO()\n",
    "        pq.write_table(table, buf, compression=\"snappy\")\n",
    "        buf.seek(0)\n",
    "        s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "\n",
    "    def check_s3_key_exists(s3_key):\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket, Key=s3_key)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    for zip_key in zip_keys:\n",
    "        print(\"Processing:\", zip_key)\n",
    "        zip_obj = s3.get_object(Bucket=bucket, Key=zip_key)[\"Body\"].read()\n",
    "\n",
    "        with zipfile.ZipFile(BytesIO(zip_obj), \"r\") as z:\n",
    "            # Filter out macOS metadata files (._filename) and only get actual CSV files\n",
    "            csv_files = [n for n in z.namelist() \n",
    "                        if n.endswith(\".csv\") and not os.path.basename(n).startswith(\"._\")]\n",
    "\n",
    "            for csv_name in csv_files:\n",
    "                # csv_name is like 'data_Q1_2024/2024-01-01.csv'\n",
    "                date_str = os.path.basename(csv_name).replace(\".csv\", \"\")\n",
    "                yyyy, mm, dd = date_str.split(\"-\")\n",
    "\n",
    "                # Write to partitioned parquet key\n",
    "                out_key = (\n",
    "                    f\"{out_prefix}year={yyyy}/month={mm}/day={dd}/\"\n",
    "                    f\"{os.path.basename(zip_key).replace('.zip','')}_{dd}.parquet\"\n",
    "                )\n",
    "\n",
    "                # Check if already extracted\n",
    "                if check_s3_key_exists(out_key):\n",
    "                    print(f\"  Skip (already exists): {out_key}\")\n",
    "                    continue\n",
    "\n",
    "                with z.open(csv_name) as f:\n",
    "                    df = pd.read_csv(f, encoding='utf-8', encoding_errors='replace')\n",
    "\n",
    "                upload_parquet_to_s3(df, out_key)\n",
    "                print(f\"  Uploaded: {out_key}\")\n",
    "\n",
    "    print(\"Done writing curated partitioned parquet to S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b16468ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U datasets pyarrow s3fs pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7465a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://None/raw/reviews_2023_parquet/raw_review_Electronics/\n",
      "s3://None/raw/reviews_2023_parquet/raw_meta_Electronics/\n"
     ]
    }
   ],
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "REVIEW_CONFIG = \"raw_review_Electronics\"\n",
    "META_CONFIG   = \"raw_meta_Electronics\"\n",
    "\n",
    "s3_reviews_out = f\"s3://{bucket}/raw/reviews_2023_parquet/{REVIEW_CONFIG}/\"\n",
    "s3_meta_out    = f\"s3://{bucket}/raw/reviews_2023_parquet/{META_CONFIG}/\"\n",
    "\n",
    "print(s3_reviews_out)\n",
    "print(s3_meta_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e8eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    from huggingface_hub import HfFileSystem\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "\n",
    "    # Initialize HF filesystem\n",
    "    hf_fs = HfFileSystem()\n",
    "\n",
    "    dataset_name = HF_DATASET\n",
    "\n",
    "    # Find Electronics review file\n",
    "    print(\"Finding Electronics review file...\")\n",
    "    review_path = f\"datasets/{dataset_name}/raw/review_categories\"\n",
    "    review_files = hf_fs.ls(review_path, detail=True)\n",
    "    electronics_files = [f for f in review_files if 'Electronics' in f['name']]\n",
    "\n",
    "    if not electronics_files:\n",
    "        print(\"No Electronics files found\")\n",
    "    else:\n",
    "        file_info = electronics_files[0]\n",
    "        file_path = file_info['name']\n",
    "        file_name = Path(file_path).name\n",
    "        \n",
    "        print(f\"Found: {file_name}\")\n",
    "        print(f\"Size: {file_info['size'] / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # First pass: Count total rows in JSONL file\n",
    "        print(f\"\\nCounting total rows in JSONL file...\")\n",
    "        total_lines = 0\n",
    "        with hf_fs.open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    total_lines += 1\n",
    "                if total_lines % 500000 == 0:\n",
    "                    print(f\"  Counted {total_lines:,} rows...\")\n",
    "        \n",
    "        print(f\"Total rows in JSONL file: {total_lines:,}\\n\")\n",
    "        \n",
    "        # Second pass: Stream JSONL from HF, convert to Parquet, upload directly to S3\n",
    "        # This avoids filling up local disk\n",
    "        # Filter for hard drive manufacturers: Western Digital, Toshiba, Seagate, Hitachi\n",
    "        print(f\"Streaming JSONL -> Parquet -> S3 (filtering for hard drive brands)...\")\n",
    "        \n",
    "        # Define target brands for filtering (case-insensitive)\n",
    "        TARGET_BRANDS = ['western digital', 'wd', 'toshiba', 'seagate', 'hitachi', 'hgst', 'wdc']\n",
    "        \n",
    "        def is_hard_drive_review(review_data):\n",
    "            \"\"\"Check if review is for Western Digital, Toshiba, Seagate, or Hitachi storage drives\"\"\"\n",
    "            # Check in title, text, and parent_asin fields\n",
    "            search_fields = []\n",
    "            \n",
    "            if 'title' in review_data and review_data['title']:\n",
    "                search_fields.append(str(review_data['title']).lower())\n",
    "            if 'text' in review_data and review_data['text']:\n",
    "                search_fields.append(str(review_data['text']).lower())\n",
    "            if 'parent_asin' in review_data:\n",
    "                search_fields.append(str(review_data['parent_asin']).lower())\n",
    "            \n",
    "            # Combine all searchable text\n",
    "            search_text = ' '.join(search_fields)\n",
    "            \n",
    "            # Check if any target brand appears in the text\n",
    "            for brand in TARGET_BRANDS:\n",
    "                if brand in search_text:\n",
    "                    # Additional check for storage-related keywords\n",
    "                    storage_keywords = ['hard drive', 'hdd', 'ssd', 'drive', 'storage', 'disk', 'external drive', 'internal drive']\n",
    "                    if any(keyword in search_text for keyword in storage_keywords):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        chunk_size = 200_000\n",
    "        part_idx = 0\n",
    "        buffer = []\n",
    "        total_rows = 0\n",
    "        filtered_rows = 0\n",
    "        \n",
    "        # Open the JSONL file from HF\n",
    "        with hf_fs.open(file_path, 'r') as f:\n",
    "            for line_num, line in enumerate(f):\n",
    "                if line.strip():\n",
    "                    review_data = json.loads(line)\n",
    "                    total_rows += 1\n",
    "                    \n",
    "                    # Filter for hard drive brands\n",
    "                    if is_hard_drive_review(review_data):\n",
    "                        buffer.append(review_data)\n",
    "                        filtered_rows += 1\n",
    "                    \n",
    "                    # When buffer reaches chunk_size, write to parquet and upload\n",
    "                    if len(buffer) >= chunk_size:\n",
    "                        df = pd.DataFrame(buffer)\n",
    "                        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                        \n",
    "                        # Write to memory buffer\n",
    "                        buf = BytesIO()\n",
    "                        pq.write_table(table, buf, compression=\"snappy\")\n",
    "                        buf.seek(0)\n",
    "                        \n",
    "                        # Upload directly to S3\n",
    "                        s3_key = s3_reviews_out.replace(f\"s3://{bucket}/\", \"\") + f\"part-{part_idx:06d}.parquet\"\n",
    "                        s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "                        \n",
    "                        print(f\"  Part {part_idx}: {len(buffer):,} rows → s3://{bucket}/{s3_key}\")\n",
    "                        \n",
    "                        # Clear buffer\n",
    "                        buffer = []\n",
    "                        part_idx += 1\n",
    "                \n",
    "                # Progress update every 100k lines\n",
    "                if (line_num + 1) % 100000 == 0:\n",
    "                    print(f\"  Processed {line_num + 1:,} lines | Filtered: {filtered_rows:,}/{total_rows:,} ({100*filtered_rows/total_rows:.2f}%)\")\n",
    "        \n",
    "        # Write remaining buffer\n",
    "        if buffer:\n",
    "            df = pd.DataFrame(buffer)\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            \n",
    "            buf = BytesIO()\n",
    "            pq.write_table(table, buf, compression=\"snappy\")\n",
    "            buf.seek(0)\n",
    "            \n",
    "            s3_key = s3_reviews_out.replace(f\"s3://{bucket}/\", \"\") + f\"part-{part_idx:06d}.parquet\"\n",
    "            s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "            \n",
    "            print(f\"  Part {part_idx}: {len(buffer):,} rows → s3://{bucket}/{s3_key}\")\n",
    "        \n",
    "        print(f\"\\n Complete!\")\n",
    "        print(f\"   Total reviews processed: {total_rows:,}\")\n",
    "        print(f\"   Hard drive reviews (WD/Toshiba/Seagate/Hitachi): {filtered_rows:,} ({100*filtered_rows/total_rows:.2f}%)\")\n",
    "        print(f\"   Parquet files created: {part_idx + 1}\")\n",
    "        print(f\"   S3 location: {s3_reviews_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaa84a",
   "metadata": {},
   "source": [
    "## Read Sample Backblaze Parquet File from S3\n",
    "\n",
    "Read a sample parquet file from the curated Backblaze data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58c1257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    from io import BytesIO\n",
    "\n",
    "    # List files in the S3 path\n",
    "    s3_path = \"curated/backblaze_parquet/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=s3_path, MaxKeys=10)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        # Get the first parquet file\n",
    "        parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if parquet_files:\n",
    "            sample_file = parquet_files[0]\n",
    "            print(f\"Reading sample file: s3://{bucket}/{sample_file}\")\n",
    "            \n",
    "            # Download and read the parquet file\n",
    "            obj = s3.get_object(Bucket=bucket, Key=sample_file)\n",
    "            buffer = BytesIO(obj['Body'].read())\n",
    "            \n",
    "            # Read into dataframe\n",
    "            df_backblaze = pd.read_parquet(buffer)\n",
    "            \n",
    "            print(f\"\\nDataframe shape: {df_backblaze.shape}\")\n",
    "            print(f\"\\nColumns: {list(df_backblaze.columns)}\")\n",
    "            print(f\"\\nFirst few rows:\")\n",
    "            display(df_backblaze.head())\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77588a13",
   "metadata": {},
   "source": [
    "## Read Sample Reviews Parquet File from S3\n",
    "\n",
    "Read a sample parquet file from the curated reviews data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b4d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # List files in the reviews S3 path\n",
    "    s3_reviews_path = \"raw/reviews_2023_parquet/raw_review_Electronics/\"\n",
    "    response_reviews = s3.list_objects_v2(Bucket=bucket, Prefix=s3_reviews_path, MaxKeys=10)\n",
    "\n",
    "    if 'Contents' in response_reviews:\n",
    "        # Get the first parquet file\n",
    "        reviews_parquet_files = [obj['Key'] for obj in response_reviews['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if reviews_parquet_files:\n",
    "            sample_reviews_file = reviews_parquet_files[0]\n",
    "            print(f\"Reading sample reviews file: s3://{bucket}/{sample_reviews_file}\")\n",
    "            \n",
    "            # Download and read the parquet file\n",
    "            obj_reviews = s3.get_object(Bucket=bucket, Key=sample_reviews_file)\n",
    "            buffer_reviews = BytesIO(obj_reviews['Body'].read())\n",
    "            \n",
    "            # Read into dataframe\n",
    "            df_reviews = pd.read_parquet(buffer_reviews)\n",
    "            \n",
    "            print(f\"\\nDataframe shape: {df_reviews.shape}\")\n",
    "            print(f\"\\nColumns: {list(df_reviews.columns)}\")\n",
    "            print(f\"\\nFirst few rows:\")\n",
    "            display(df_reviews.head())\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_reviews_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_reviews_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63456818",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Count unique ASIN values\n",
    "    unique_asin_count = df_reviews['asin'].nunique()\n",
    "    print(f\"Unique ASIN values: {unique_asin_count:,}\")\n",
    "\n",
    "    # Also show unique parent_asin if it exists\n",
    "    if 'parent_asin' in df_reviews.columns:\n",
    "        unique_parent_asin_count = df_reviews['parent_asin'].nunique()\n",
    "        print(f\"Unique parent_asin values: {unique_parent_asin_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859957ba",
   "metadata": {},
   "source": [
    "## Examine Data for Joining\n",
    "\n",
    "Check sample values from both datasets to understand how to create a join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146e589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Sample review titles and parent_asin to see product names\n",
    "    print(\"Sample Reviews Data:\")\n",
    "    print(df_reviews[['parent_asin', 'title', 'text']].head(10))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Sample backblaze model names\n",
    "    print(\"Sample Backblaze Model Data:\")\n",
    "    print(df_backblaze['model'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1c9a",
   "metadata": {},
   "source": [
    "## Create Join Keys\n",
    "\n",
    "Extract manufacturer and model information from reviews to match with Backblaze model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "189bd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    import re\n",
    "\n",
    "    def extract_manufacturer(text):\n",
    "        \"\"\"Extract manufacturer from review text\"\"\"\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Map various manufacturer names to standard format\n",
    "        if 'toshiba' in text_lower:\n",
    "            return 'TOSHIBA'\n",
    "        elif 'seagate' in text_lower or 'st' in text_lower[:3]:\n",
    "            return 'SEAGATE'\n",
    "        elif 'western digital' in text_lower or 'wd' in text_lower or 'wdc' in text_lower:\n",
    "            return 'WDC'\n",
    "        elif 'hitachi' in text_lower or 'hgst' in text_lower:\n",
    "            return 'HGST'\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def extract_model_hints(text):\n",
    "        \"\"\"Extract potential model numbers/patterns from review text\"\"\"\n",
    "        text_upper = str(text).upper()\n",
    "        \n",
    "        # Look for common model patterns\n",
    "        patterns = [\n",
    "            r'[A-Z]{2,}\\s*[A-Z0-9]{6,}',  # e.g., \"MG08ACA16TA\", \"WUH722222ALE6L4\"\n",
    "            r'ST\\d{4,}[A-Z]{2}\\d{3,}[A-Z]?',  # Seagate pattern\n",
    "            r'WD[A-Z0-9]{6,}',  # WD pattern\n",
    "            r'MG\\d{2}[A-Z]{3}\\d{2}[A-Z]{2,}',  # Toshiba pattern\n",
    "        ]\n",
    "        \n",
    "        models = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text_upper)\n",
    "            models.extend(matches)\n",
    "        \n",
    "        return models if models else None\n",
    "\n",
    "    # Add manufacturer and model hints to reviews dataframe\n",
    "    df_reviews['manufacturer'] = df_reviews['title'].apply(extract_manufacturer)\n",
    "    df_reviews['manufacturer'] = df_reviews['manufacturer'].fillna(\n",
    "        df_reviews['text'].apply(extract_manufacturer)\n",
    "    )\n",
    "\n",
    "    df_reviews['model_hints'] = df_reviews['title'].apply(extract_model_hints)\n",
    "    df_reviews['model_hints'] = df_reviews['model_hints'].fillna(\n",
    "        df_reviews['text'].apply(extract_model_hints)\n",
    "    )\n",
    "\n",
    "    # Add manufacturer to backblaze data for easier joining\n",
    "    def get_bb_manufacturer(model):\n",
    "        \"\"\"Extract manufacturer from Backblaze model name\"\"\"\n",
    "        if model.startswith('TOSHIBA'):\n",
    "            return 'TOSHIBA'\n",
    "        elif model.startswith('ST') or model.startswith('SEAGATE'):\n",
    "            return 'SEAGATE'\n",
    "        elif model.startswith('WDC') or model.startswith('WD'):\n",
    "            return 'WDC'\n",
    "        elif model.startswith('HGST') or model.startswith('HITACHI'):\n",
    "            return 'HGST'\n",
    "        return None\n",
    "\n",
    "    df_backblaze['manufacturer'] = df_backblaze['model'].apply(get_bb_manufacturer)\n",
    "\n",
    "    print(\"Reviews with manufacturer extracted:\")\n",
    "    print(df_reviews[['parent_asin', 'title', 'manufacturer', 'model_hints']].head(10))\n",
    "    print(f\"\\nReviews with manufacturer identified: {df_reviews['manufacturer'].notna().sum():,} / {len(df_reviews):,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"Backblaze with manufacturer extracted:\")\n",
    "    print(df_backblaze[['model', 'manufacturer']].head(10))\n",
    "    print(f\"\\nManufacturer distribution in Backblaze:\")\n",
    "    print(df_backblaze['manufacturer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c4625",
   "metadata": {},
   "source": [
    "## Join Strategy Summary\n",
    "\n",
    "Now both datasets have a `manufacturer` field that can be used for joining:\n",
    "\n",
    "- **Reviews**: `df_reviews['manufacturer']` - extracted from title/text (TOSHIBA, SEAGATE, WDC, HGST)\n",
    "- **Backblaze**: `df_backblaze['manufacturer']` - extracted from model name\n",
    "\n",
    "You can join on manufacturer to analyze reviews by manufacturer against Backblaze failure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca1817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Example join: Get reviews by manufacturer\n",
    "    print(\"Join Example - Reviews by Manufacturer:\")\n",
    "    print(\"\\nReviews distribution:\")\n",
    "    print(df_reviews['manufacturer'].value_counts())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"Backblaze distribution:\")\n",
    "    print(df_backblaze['manufacturer'].value_counts())\n",
    "\n",
    "    # You can now join like this:\n",
    "    # joined_df = df_reviews.merge(df_backblaze, on='manufacturer', how='inner')\n",
    "    # Or do aggregations by manufacturer before joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89637555",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Check a sample Backblaze parquet file to understand the schema\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    from io import BytesIO\n",
    "\n",
    "    # List some parquet files\n",
    "    s3_path = \"curated/backblaze_parquet/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=s3_path, MaxKeys=100)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        # Find the first actual parquet file\n",
    "        parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if parquet_files:\n",
    "            sample_file = parquet_files[0]\n",
    "            print(f\"Reading schema from: {sample_file}\\n\")\n",
    "            \n",
    "            # Download and read parquet file metadata\n",
    "            obj = s3.get_object(Bucket=bucket, Key=sample_file)\n",
    "            buffer = BytesIO(obj['Body'].read())\n",
    "            \n",
    "            # Read parquet schema\n",
    "            parquet_file = pq.ParquetFile(buffer)\n",
    "            arrow_schema = parquet_file.schema_arrow\n",
    "            \n",
    "            print(f\"Number of columns: {len(arrow_schema.names)}\\n\")\n",
    "            print(\"First 20 columns:\")\n",
    "            for i, name in enumerate(arrow_schema.names[:20], 1):\n",
    "                field = arrow_schema.field(name)\n",
    "                print(f\"  {i}. {name} ({field.type})\")\n",
    "            \n",
    "            print(f\"\\n... and {len(arrow_schema.names) - 20} more columns\")\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
